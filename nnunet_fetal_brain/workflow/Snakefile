#---- begin snakebids boilerplate ----------------------------------------------

from glob import glob
import snakebids
from snakebids import bids

configfile: 'config/snakebids.yml'

train=True

if train:
    participant_label=config['train_subjects']
else:
    participant_label=config['participant_label']


#writes inputs_config.yml and updates config dict
config.update(
    snakebids.generate_inputs(
        bids_dir=config["bids_dir"],
        pybids_inputs=config["pybids_inputs"],
        derivatives=config["derivatives"],
        participant_label=participant_label,
        exclude_participant_label=config["exclude_participant_label"]
    )
)



#this adds constraints to the bids naming
wildcard_constraints:  **snakebids.get_wildcard_constraints(\
    config["pybids_inputs"]\
)

#---- end snakebids boilerplate ------------------------------------------------

model = config['use_downloaded']




localrules: download_model


rule cleanorient_bold:
    input: 
        nii = config['input_path']['bold']
    output: 
        nii = bids(root='results', datatype='func',
                  desc='cleanorient', suffix='bold.nii.gz',
                  **config['input_wildcards']['bold'])
    shell: 
        'c4d {input} -o {output} && fslorient -deleteorient {output}'


rule cleanorient_mask:
    input: 
        nii = config['input_path']['mask']
    output: 
        nii = bids(root='results', datatype='func',
                  desc='cleanorient', suffix='mask.nii.gz',
                  **config['input_wildcards']['mask'])
    shell: 
        'c4d {input} -foreach -binarize -endfor -o {output} && fslorient -deleteorient {output}'



rule split_cleanorient_bold:
    input: 
        nii = bids(root='results', datatype='func',
                  desc='cleanorient', suffix='bold.nii.gz',
                  **config['input_wildcards']['bold'])
    params:
        img_prefix = bids(suffix='',include_subject_dir=False,
                            include_session_dir=False,
                            **config['input_wildcards']['bold']),
    output: 
        split_dir = directory(bids(root='results', datatype='func',
                  desc='cleanorient', suffix='bold',
                  **config['input_wildcards']['bold']))
    container: '/project/6050199/akhanf/singularity/bids-apps/khanlab_neuroglia-core_latest.sif'
    group: 'subj'
    shell: 
        #split, then replace suffix with _0000.nii.gz 
        'mkdir -p {output} && fslsplit {input}  {output}/{params.img_prefix} && '
        'for im in `ls {output}/*.nii.gz`; do '
        'c3d $im -pad-to 96x96x37 -o ${{im%%.nii.gz}}_0000.nii.gz &&  rm -f $im; '  #need to pad to 96x96x37 as some datasets in rutherford have incorrect cropping..
        'fslorient -deleteorient ${{im%%.nii.gz}}_0000.nii.gz ; '
        'done'


rule split_cleanorient_mask:
    input: 
        nii = bids(root='results', datatype='func',
                  desc='cleanorient', suffix='mask.nii.gz',
                  **config['input_wildcards']['mask'])
    params:
        img_prefix = bids(suffix='',include_subject_dir=False,
                            include_session_dir=False,
                            **config['input_wildcards']['mask']),
    output: 
        split_dir = directory(bids(root='results', datatype='func',
                  desc='cleanorient', suffix='mask',
                  **config['input_wildcards']['mask']))
    container: '/project/6050199/akhanf/singularity/bids-apps/khanlab_neuroglia-core_latest.sif'
    group: 'subj'
    shell: 'mkdir -p {output} && fslsplit {input}  {output}/{params.img_prefix}'






checkpoint cp_training_img:
    input: 
        split_dirs = expand(bids(root='results', datatype='func',
                      desc='cleanorient', suffix='bold',
                      **config['input_wildcards']['bold']),
                        zip,
                        **config['input_zip_lists']['bold'])
    output: 
        training_img_dir = directory('results/nnUNet_raw_data/{unettask}/imagesTr'.format(
                                unettask = config['download_model'][model]['unettask']))
    threads: 32 #to make it serial on a node
    group: 'preproc'
    shell: 'mkdir -p {output} && '
           'for dir in {input.split_dirs}; do'
           '  cp -v ${{dir}}/*.nii.gz {output.training_img_dir}; '
           'done'


checkpoint cp_training_lbl:
    input: 
        split_dirs = expand(bids(root='results', datatype='func',
                      desc='cleanorient', suffix='mask',
                      **config['input_wildcards']['mask']),
                        zip,
                        **config['input_zip_lists']['bold'])
    output: 
        training_img_dir = directory('results/nnUNet_raw_data/{unettask}/labelsTr'.format(
                                unettask = config['download_model'][model]['unettask']))
    threads: 32 #to make it serial on a node
    group: 'preproc'
    shell: 'mkdir -p {output} && '
           'for dir in {input.split_dirs}; do'
           '  cp -v ${{dir}}/*.nii.gz {output.training_img_dir}; '
           'done'

def get_training_imgs(wildcards):
    checkpoint_output = checkpoints.cp_training_img.get(**wildcards).output[0]
    return sorted(glob(os.path.join(checkpoint_output,'*.nii.gz')))

def get_training_lbls(wildcards):
    checkpoint_output = checkpoints.cp_training_lbl.get(**wildcards).output[0]
    return sorted(glob(os.path.join(checkpoint_output,'*.nii.gz')))



def get_training_imgs_nosuffix(wildcards,input):
    return [img[:-12]+'.nii.gz' for img in input.training_imgs]


rule create_dataset_json:
    input: 
        training_imgs = get_training_imgs,
        training_lbls = get_training_lbls,
        template_json = os.path.join(config['snakemake_dir'],'resources/template.json')
    params:
        training_imgs_nosuffix = get_training_imgs_nosuffix
    output: 
        dataset_json = 'results/nnUNet_raw_data/{unettask}/dataset.json'.format(
                                unettask = config['download_model'][model]['unettask'])
            
    group: 'preproc'
    script: 'scripts/create_json.py' 
 

def get_nnunet_env(wildcards):
     return ' && '.join([f'export {key}={val}' for (key,val) in config['nnunet_env'].items()])
 
def get_nnunet_env_tmp(wildcards):
     return ' && '.join([f'export {key}={val}' for (key,val) in config['nnunet_env_tmp'].items()])
 
rule plan_preprocess:
    input: 
        dataset_json = 'results/nnUNet_raw_data/{unettask}/dataset.json'.format(
                                unettask = config['download_model'][model]['unettask'])
 
    params:
        nnunet_env_cmd = get_nnunet_env,
        task_num = lambda wildcards: re.search('Task([0-9]+)\w*',config['download_model'][model]['unettask']).group(1),
    output: 
        dataset_json = 'preprocessed/{unettask}/dataset.json'.format(
                                unettask = config['download_model'][model]['unettask'])
    group: 'preproc'
    resources:
        threads = 8,
        mem_mb = 16000
    container: '/project/6050199/akhanf/singularity/bids-apps/khanlab_hippunfold_v0.5.1.sif'
    shell:
        '{params.nnunet_env_cmd} && '
        'nnUNet_plan_and_preprocess  -t {params.task_num} --verify_dataset_integrity'


def get_checkpoint_opt(wildcards, output):
    if os.path.exists(output.latest_model):
        return '--continue_training'
    else:
        return '' 
      
rule train_fold:
    input:
        dataset_json = 'preprocessed/{unettask}/dataset.json'.format(
                                unettask = config['download_model'][model]['unettask'])
    params:
        nnunet_env_cmd = get_nnunet_env_tmp,
        rsync_to_tmp = f"rsync -av {config['nnunet_env']['nnUNet_preprocessed']} $SLURM_TMPDIR",
        #add --continue_training option if a checkpoint exists
        checkpoint_opt = get_checkpoint_opt,
        unettask=config['download_model'][model]['unettask'],
        trainer=config['nnunet']['trainer'],
        arch= config['nnunet']['arch']

    output:
        latest_model = 'trained_models/nnUNet/{arch}/{unettask}/{trainer}__nnUNetPlansv2.1/fold_{{fold}}/model_latest.model'.format(
                            unettask=config['download_model'][model]['unettask'],
                            trainer=config['nnunet']['trainer'],
                            arch= config['nnunet']['arch']),
               
        best_model = 'trained_models/nnUNet/{arch}/{unettask}/{trainer}__nnUNetPlansv2.1/fold_{{fold}}/model_best.model'.format(
                            unettask=config['download_model'][model]['unettask'],
                            trainer=config['nnunet']['trainer'],
                            arch= config['nnunet']['arch'])
    threads: 16
    resources:
        gpus = 1,
        mem_mb = 64000,
        time = 1440,
    group: 'train'
    container: '/project/6050199/akhanf/singularity/bids-apps/khanlab_hippunfold_v0.5.1.sif'
    shell:
        '{params.nnunet_env_cmd} && '
        '{params.rsync_to_tmp} && '
        'nnUNet_train {params.checkpoint_opt} {params.arch} {params.trainer} {params.unettask} {wildcards.fold}'



rule all_train:
    input:
       expand('trained_models/nnUNet/{arch}/{unettask}/{trainer}__nnUNetPlansv2.1/fold_{fold}/{checkpoint}.model',
                    fold=range(5), 
                    checkpoint = config['download_model'][model]['checkpoint'],
                    unettask = config['download_model'][model]['unettask'],
                    arch=config['nnunet']['arch'], 
                    trainer=config['nnunet']['trainer'])

rule all_test:
    input:
        nii = expand(bids(root='results', datatype='func',
                  desc='brain', suffix='mask.nii.gz',
                  **config['input_wildcards']['bold']),
                        zip,
                        **config['input_zip_lists']['bold'])



rule split:
    input: config['input_path']['bold']
    output: 
        split_dir = directory(bids(root='results', datatype='func',
                  desc='split', suffix='bold',
                  **config['input_wildcards']['bold']))
    container: '/project/6050199/akhanf/singularity/bids-apps/khanlab_neuroglia-core_latest.sif'
    group: 'subj'
    shell: 'mkdir -p {output} && fslsplit {input}  {output}/vol_'


rule conform:
    input: 
        nii_dir = bids(root='results', datatype='func',
                  desc='split', suffix='bold',
                  **config['input_wildcards']['bold'])
    params:
        resample_mm = '3.5x3.5x3.5mm',
        pad_to = '96x96x37',
    output: 
        nii_dir = directory(bids(root='results', datatype='func',
                  desc='conform', suffix='bold',
                  **config['input_wildcards']['bold']))
 
    container: '/project/6050199/akhanf/singularity/bids-apps/khanlab_autotop_deps_v0.4.1.sif'
    group: 'subj'
    shell: 'mkdir -p {output} && '
            'for in_nii in `ls {input}/*.nii.gz`; do '
            ' filename=${{in_nii##*/}} && '
            ' prefix=${{filename%%.nii.gz}} &&'
            ' out_nii={output}/${{prefix}}_0000.nii.gz && ' #always append _0000.nii.gz for nnunet
            ' c3d $in_nii -resample-mm {params.resample_mm} -pad-to {params.pad_to} 0 $out_nii;' 
            'done'





rule download_model:
    params: 
        url = config['download_model'][model]['url']
    output: os.path.join('resources',config['download_model'][model]['tar'])
    shell: 'pushd resources && wget {params.url}'

rule extract_model:
    input: os.path.join('resources',config['download_model'][model]['tar'])
    output: 
        models = expand(os.path.join('resources','trained_models',config['download_model'][model]['out']),fold=range(5)),
    shell: 'mkdir -p resources/trained_model && tar -C resources/trained_models -xvf {input}'


rule run_inference:
    input:
        nii_dir = bids(root='results', datatype='func',
                  desc='conform', suffix='bold',
                  **config['input_wildcards']['bold']),
        model_tar = os.path.join('resources',config['download_model'][model]['tar'])
    output:
        nii_dir = directory(
                    bids(root='results', datatype='func',
                        desc='brain', suffix='mask',
                        **config['input_wildcards']['bold']))
    threads: 8
    resources:
        mem_mb = 32000,
        time = 30,
        augmentation_threads = 4,
    group: 'subj'
    shadow: 'minimal'
    params:
        temp_img = 'tempimg/temp_0000.nii.gz',
        temp_lbl = 'templbl/temp.nii.gz',
        model_dir = 'tempmodel',
        in_folder = 'tempimg',
        out_folder = 'templbl',
        chkpnt = config['download_model'][model]['checkpoint'],
        unettask = config['download_model'][model]['unettask'],
    container: '/project/6050199/akhanf/singularity/bids-apps/khanlab_hippunfold_v0.5.1.sif'
    shell: 'mkdir -p {params.model_dir} {params.in_folder} {params.out_folder} {output.nii_dir} && ' #create temp folders
           'cp -v {input.nii_dir}/*.nii.gz {params.in_folder} && ' #cp input image to temp folder
           'tar -xvf {input.model_tar} -C {params.model_dir} && ' #extract model
           'export RESULTS_FOLDER={params.model_dir} && ' #set nnunet env var to point to model
           'export nnUNet_n_proc_DA={resources.augmentation_threads} && ' #set threads
           'nnUNet_predict -i {params.in_folder} -o {params.out_folder} '
           ' -t {params.unettask} -chk {params.chkpnt} && ' # run inference
           'cp -v {params.out_folder}/*.nii.gz {output.nii_dir}' #copy from temp output folder to final output



rule merge_mask:
    input:
        nii_dir = bids(root='results', datatype='func',
                  desc='brain', suffix='mask',
                  **config['input_wildcards']['bold'])
    output:
        nii = bids(root='results', datatype='func',
                  desc='brain', suffix='mask.nii.gz',
                  **config['input_wildcards']['bold'])
    group: 'subj'
    log: bids(root='logs',**config['input_wildcards']['bold'],suffix='merge.txt')
    container: '/project/6050199/akhanf/singularity/bids-apps/khanlab_neuroglia-core_latest.sif'
    shell: 
        'fslmerge -t {output} {input}/*.nii.gz'


